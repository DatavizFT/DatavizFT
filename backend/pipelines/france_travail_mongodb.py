"""
Pipeline MongoDB France Travail M1805 - Version adapt√©e
Pipeline utilisant MongoDB au lieu des fichiers JSON pour le stockage
"""

import asyncio
from datetime import datetime, timedelta
from typing import Any

from motor.motor_asyncio import AsyncIOMotorDatabase

from ..clients.france_travail import FranceTravailAPIClient
from ..config import Config
from ..data import COMPETENCES_REFERENTIEL
from ..database.repositories import (
    CompetencesRepository,
    OffresRepository,
    StatsRepository,
)
from ..models.mongodb.schemas import OffreEmploiModel
from ..tools.competence_analyzer import CompetenceAnalyzer
from ..tools.logging_config import get_logger

logger = get_logger(__name__)


class PipelineMongoDBM1805:
    """Pipeline MongoDB pour collecte et analyse des offres M1805"""

    def __init__(self):
        """Initialise le pipeline MongoDB"""
        self.config = Config()
        self.code_rome = "M1805"
        self.description = "√âtudes et d√©veloppement informatique"

        # R√©f√©rentiel de comp√©tences
        self.competences_referentiel = COMPETENCES_REFERENTIEL

        # Clients et outils
        self.api_client = FranceTravailAPIClient()
        self.analyzer = CompetenceAnalyzer(self.competences_referentiel)

        # Base de donn√©es (√† initialiser)
        self.db: AsyncIOMotorDatabase | None = None
        self.offres_repo: OffresRepository | None = None
        self.competences_repo: CompetencesRepository | None = None
        self.stats_repo: StatsRepository | None = None

        # √âtat des connexions
        self._connections_closed = False

        logger.info(
            f"Pipeline MongoDB M1805 initialis√© - {len(self.competences_referentiel)} cat√©gories de comp√©tences"
        )

    async def init_database_connection(self) -> bool:
        """
        Initialise la connexion directe MongoDB

        Returns:
            True si succ√®s, False sinon
        """
        try:
            # Connexion directe avec Motor
            import motor.motor_asyncio

            self.client_mongo = motor.motor_asyncio.AsyncIOMotorClient(
                self.config.MONGODB_URL
            )
            self.db = self.client_mongo[self.config.MONGODB_DATABASE]

            # Test de connexion
            await self.client_mongo.admin.command("ping")

            # Initialiser les repositories
            self.offres_repo = OffresRepository(self.db)
            self.competences_repo = CompetencesRepository(self.db)
            self.stats_repo = StatsRepository(self.db)

            # Initialiser l'analyseur de comp√©tences
            self.analyseur_competences = CompetenceAnalyzer(
                self.competences_referentiel
            )

            # Initialiser le client API
            self.client_api = FranceTravailAPIClient()

            logger.info("‚úÖ Connexion MongoDB directe √©tablie")
            return True

        except Exception as e:
            logger.error(f"‚ùå Erreur connexion MongoDB directe: {e}")
            return False

    async def close_database_connection(self):
        """Ferme la connexion MongoDB"""
        if hasattr(self, "_connections_closed") and self._connections_closed:
            logger.debug("Connexions d√©j√† ferm√©es, ignor√©")
            return

        try:
            # Fermeture client API (synchrone)
            if hasattr(self, "client_api") and self.client_api:
                self.client_api.close()  # Pas de await - m√©thode sync
                self.client_api = None

            # Fermeture client MongoDB (synchrone aussi)
            if hasattr(self, "client_mongo") and self.client_mongo:
                self.client_mongo.close()
                self.client_mongo = None

            # Marquer comme ferm√© pour √©viter les doubles appels
            self._connections_closed = True
            logger.info("üîå Connexions ferm√©es proprement")

        except Exception as e:
            logger.warning(f"Erreur fermeture connexions: {e}")

    async def verifier_derniere_execution(self) -> dict[str, Any]:
        """
        V√©rifie si le pipeline a √©t√© ex√©cut√© dans les 24 derni√®res heures
        Utilise MongoDB au lieu des fichiers

        Returns:
            Dict contenant le statut et les informations de la derni√®re ex√©cution
        """
        try:
            # Chercher les offres les plus r√©centes dans MongoDB
            offres_recentes = await self.offres_repo.get_offres_recentes(
                jours=1, limit=1
            )

            if not offres_recentes:
                return {
                    "doit_executer": True,
                    "raison": "Aucune offre r√©cente dans MongoDB",
                    "derniere_execution": None,
                    "nb_offres_recentes": 0,
                }

            derniere_offre = offres_recentes[0]
            derniere_execution = derniere_offre.get("date_creation")

            if not derniere_execution:
                return {
                    "doit_executer": True,
                    "raison": "Pas de date de cr√©ation trouv√©e",
                    "derniere_execution": None,
                    "nb_offres_recentes": len(offres_recentes),
                }

            # V√©rifier si c'est dans les 24 derni√®res heures
            maintenant = datetime.now()
            if isinstance(derniere_execution, str):
                derniere_execution = datetime.fromisoformat(derniere_execution)

            delta = maintenant - derniere_execution

            # Compter les offres des 24 derni√®res heures
            stats_24h = await self.offres_repo.get_collection_stats()

            if delta <= timedelta(hours=24):
                return {
                    "doit_executer": False,
                    "raison": f"Donn√©es r√©centes trouv√©es (il y a {delta.total_seconds()/3600:.1f}h)",
                    "derniere_execution": derniere_execution,
                    "nb_offres_24h": stats_24h.get("total_offres", 0),
                }
            else:
                return {
                    "doit_executer": True,
                    "raison": f"Donn√©es obsol√®tes (il y a {delta.days} jours)",
                    "derniere_execution": derniere_execution,
                    "nb_offres_24h": stats_24h.get("total_offres", 0),
                }

        except Exception as e:
            logger.error(f"Erreur v√©rification derni√®re ex√©cution: {e}")
            return {
                "doit_executer": True,
                "raison": f"Erreur v√©rification: {e}",
                "derniere_execution": None,
                "nb_offres_recentes": 0,
            }

    async def verifier_derniere_analyse_competences(self) -> dict[str, Any]:
        """
        V√©rifie si l'analyse des comp√©tences a √©t√© effectu√©e dans les 24 derni√®res heures

        Returns:
            Dict contenant le statut et les informations de la derni√®re analyse
        """
        try:
            # Chercher la derni√®re ex√©cution de l'analyse des comp√©tences
            derniere_execution = await self.db.pipeline_executions.find_one(
                {"type": "analyse_competences"}
            )

            if not derniere_execution:
                return {
                    "doit_analyser": True,
                    "raison": "Aucune analyse de comp√©tences pr√©c√©dente trouv√©e",
                    "derniere_analyse": None,
                }

            derniere_analyse = derniere_execution.get("derniere_execution")
            if not derniere_analyse:
                return {
                    "doit_analyser": True,
                    "raison": "Date d'analyse manquante",
                    "derniere_analyse": None,
                }

            # V√©rifier si c'est dans les 24 derni√®res heures
            maintenant = datetime.now()
            if isinstance(derniere_analyse, str):
                derniere_analyse = datetime.fromisoformat(derniere_analyse)

            delta = maintenant - derniere_analyse

            if delta <= timedelta(hours=24):
                return {
                    "doit_analyser": False,
                    "raison": f"Analyse r√©cente trouv√©e (il y a {delta.total_seconds()/3600:.1f}h)",
                    "derniere_analyse": derniere_analyse,
                }
            else:
                return {
                    "doit_analyser": True,
                    "raison": f"Analyse obsol√®te (il y a {delta.days} jours)",
                    "derniere_analyse": derniere_analyse,
                }

        except Exception as e:
            logger.error(f"Erreur v√©rification derni√®re analyse: {e}")
            return {
                "doit_analyser": True,
                "raison": f"Erreur v√©rification: {e}",
                "derniere_analyse": None,
            }

    async def verifier_dernieres_statistiques(self) -> dict[str, Any]:
        """
        V√©rifie si les statistiques ont √©t√© g√©n√©r√©es dans les 24 derni√®res heures

        Returns:
            Dict contenant le statut et les informations des derni√®res statistiques
        """
        try:
            # Chercher la derni√®re ex√©cution des statistiques
            derniere_execution = await self.db.pipeline_executions.find_one(
                {"type": "statistiques"}
            )

            if not derniere_execution:
                return {
                    "doit_generer": True,
                    "raison": "Aucune statistique pr√©c√©dente trouv√©e",
                    "derniere_generation": None,
                }

            derniere_generation = derniere_execution.get("derniere_execution")
            if not derniere_generation:
                return {
                    "doit_generer": True,
                    "raison": "Date de g√©n√©ration manquante",
                    "derniere_generation": None,
                }

            # V√©rifier si c'est dans les 24 derni√®res heures
            maintenant = datetime.now()
            if isinstance(derniere_generation, str):
                derniere_generation = datetime.fromisoformat(derniere_generation)

            delta = maintenant - derniere_generation

            if delta <= timedelta(hours=24):
                return {
                    "doit_generer": False,
                    "raison": f"Statistiques r√©centes trouv√©es (il y a {delta.total_seconds()/3600:.1f}h)",
                    "derniere_generation": derniere_generation,
                }
            else:
                return {
                    "doit_generer": True,
                    "raison": f"Statistiques obsol√®tes (il y a {delta.days} jours)",
                    "derniere_generation": derniere_generation,
                }

        except Exception as e:
            logger.error(f"Erreur v√©rification derni√®res statistiques: {e}")
            return {
                "doit_generer": True,
                "raison": f"Erreur v√©rification: {e}",
                "derniere_generation": None,
            }

    async def collecter_offres(self, max_offres: int | None = None) -> list[dict]:
        """
        Collecte les offres d'emploi M1805 via l'API France Travail

        Args:
            max_offres: Limite optionnelle du nombre d'offres

        Returns:
            Liste des offres collect√©es
        """
        logger.info(f"D√©but collecte offres {self.code_rome}")

        try:
            offres = self.api_client.collecter_offres_par_code_rome(
                self.code_rome, max_offres=max_offres
            )

            logger.info(f"{len(offres)} offres collect√©es")
            return offres

        except Exception as e:
            logger.error(f"Erreur collecte offres: {e}")
            return []

    async def convertir_et_sauvegarder_offres(self, offres_api: list[dict]) -> int:
        """
        Convertit les offres API en mod√®les MongoDB et les sauvegarde

        Args:
            offres_api: Liste des offres brutes de l'API

        Returns:
            Nombre d'offres sauvegard√©es
        """
        logger.info("Conversion et sauvegarde des offres dans MongoDB")

        offres_converties = []
        erreurs_conversion = 0

        for offre_api in offres_api:
            try:
                # Convertir l'offre API en mod√®le MongoDB
                offre_modele = await self._convertir_offre_api_vers_modele(offre_api)
                offres_converties.append(offre_modele)

            except Exception as e:
                erreurs_conversion += 1
                logger.warning(
                    f"Erreur conversion offre {offre_api.get('id', 'N/A')}: {e}"
                )

        if erreurs_conversion > 0:
            logger.warning(f"{erreurs_conversion} offres non converties")

        # Sauvegarde en lot dans MongoDB avec d√©duplication
        if offres_converties:
            logger.info(f"Tentative de sauvegarde de {len(offres_converties)} offres")
            nb_sauvegardees = await self.offres_repo.insert_many_offres(
                offres_converties
            )

            doublons = len(offres_converties) - nb_sauvegardees
            if doublons > 0:
                logger.info(
                    f"{nb_sauvegardees} nouvelles offres sauvegard√©es, {doublons} doublons ignor√©s"
                )
            else:
                logger.info(f"{nb_sauvegardees} offres sauvegard√©es dans MongoDB")

            return nb_sauvegardees
        else:
            logger.warning("Aucune offre √† sauvegarder")
            return 0

    async def _convertir_offre_api_vers_modele(
        self, offre_api: dict
    ) -> OffreEmploiModel:
        """
        Convertit une offre API en mod√®le OffreEmploiModel pour MongoDB
        PR√âSERVE TOUS les champs de l'API France Travail sans perte de donn√©es

        Args:
            offre_api: Offre brute de l'API France Travail

        Returns:
            Mod√®le OffreEmploiModel avec TOUTES les donn√©es de l'API
        """
        # COPIE INT√âGRALE de l'offre API - aucune perte de donn√©es
        donnees_api_completes = dict(offre_api)  # Copie compl√®te

        # Extraction s√©lective pour les champs sp√©ciaux du mod√®le normalis√©
        return OffreEmploiModel(
            # Identifiants et base
            source_id=str(offre_api.get("id", "")),
            intitule=offre_api.get("intitule", ""),
            description=offre_api.get("description", ""),
            # Dates avec gestion des formats
            date_creation=self._parse_date_api(offre_api.get("dateCreation")),
            date_mise_a_jour=self._parse_date_api(offre_api.get("dateActualisation")),
            # DONN√âES API COMPL√àTES - Conservation int√©grale
            donnees_api_originales=donnees_api_completes,
            # Entreprise (extraction pour compatibilit√©)
            entreprise={
                "nom": offre_api.get("entreprise", {}).get("nom", ""),
                "description": offre_api.get("entreprise", {}).get("description", ""),
                "logo": offre_api.get("entreprise", {}).get("logo", ""),
                "url": offre_api.get("entreprise", {}).get("url", ""),
                "entrepriseAdaptee": offre_api.get("entreprise", {}).get(
                    "entrepriseAdaptee"
                ),
            },
            # Localisation compl√®te avec coordonn√©es GPS
            localisation={
                "libelle": offre_api.get("lieuTravail", {}).get("libelle", ""),
                "latitude": offre_api.get("lieuTravail", {}).get("latitude"),
                "longitude": offre_api.get("lieuTravail", {}).get("longitude"),
                "codePostal": offre_api.get("lieuTravail", {}).get("codePostal", ""),
                "commune": offre_api.get("lieuTravail", {}).get("commune", ""),
                "departement": (
                    offre_api.get("lieuTravail", {}).get("codePostal", "")[:2]
                    if offre_api.get("lieuTravail", {}).get("codePostal")
                    else ""
                ),
            },
            # Contrat complet
            contrat={
                "type": offre_api.get("typeContrat", ""),
                "typeLibelle": offre_api.get("typeContratLibelle", ""),
                "natureContrat": offre_api.get("natureContrat", ""),
                "experienceExige": offre_api.get("experienceExige", ""),
                "experienceLibelle": offre_api.get("experienceLibelle", ""),
                "experienceCommentaire": offre_api.get("experienceCommentaire", ""),
                "dureeTravailLibelle": offre_api.get("dureeTravailLibelle", ""),
                "dureeTravailLibelleConverti": offre_api.get(
                    "dureeTravailLibelleConverti", ""
                ),
                "alternance": offre_api.get("alternance", False),
            },
            # Codes et m√©tier
            rome_code=offre_api.get("romeCode", self.code_rome),
            rome_libelle=offre_api.get("romeLibelle", ""),
            appellation_libelle=offre_api.get("appellationlibelle", ""),
            # Secteur d'activit√©
            code_naf=offre_api.get("codeNAF", ""),
            secteur_activite=offre_api.get("secteurActivite", ""),
            secteur_activite_libelle=offre_api.get("secteurActiviteLibelle", ""),
            # Qualification
            qualification_code=offre_api.get("qualificationCode", ""),
            qualification_libelle=offre_api.get("qualificationLibelle", ""),
            # Salaire complet
            salaire=offre_api.get("salaire", {}),
            # Formations, langues, permis, comp√©tences (listes compl√®tes)
            formations=offre_api.get("formations", []),
            langues=offre_api.get("langues", []),
            permis=offre_api.get("permis", []),
            competences_requises=offre_api.get("competences", []),
            qualites_professionnelles=offre_api.get("qualitesProfessionnelles", []),
            outils_bureautiques=offre_api.get("outilsBureautiques", ""),
            # Contact et agence
            contact=offre_api.get("contact", {}),
            agence=offre_api.get("agence", {}),
            # M√©tadonn√©es √©tendues
            nombre_postes=offre_api.get("nombrePostes", 1),
            accessible_th=offre_api.get("accessibleTH", False),
            deplacement_code=offre_api.get("deplacementCode", ""),
            deplacement_libelle=offre_api.get("deplacementLibelle", ""),
            tranche_effectif_etab=offre_api.get("trancheEffectifEtab", ""),
            entreprise_adaptee=offre_api.get("entrepriseAdaptee", False),
            employeur_handi_engage=offre_api.get("employeurHandiEngage", False),
            # Origine et contexte
            origine_offre=offre_api.get("origineOffre", {}),
            offres_manque_candidats=offre_api.get("offresManqueCandidats", False),
            contexte_travail=offre_api.get("contexteTravail", {}),
            complement_exercice=offre_api.get("complementExercice", ""),
            condition_exercice=offre_api.get("conditionExercice", ""),
            # Comp√©tences (√† analyser plus tard)
            competences_extraites=[],
            # M√©tadonn√©es syst√®me
            code_rome=self.code_rome,
            source="france_travail_api",
            traite=False,
        )

    def _parse_date_api(self, date_str: str | None) -> datetime | None:
        """
        Parse les dates de l'API France Travail

        Args:
            date_str: Date sous forme de cha√Æne

        Returns:
            DateTime pars√© ou None
        """
        if not date_str:
            return None

        try:
            # Format API France Travail: "2024-10-15T10:30:00.000Z"
            if "T" in date_str:
                date_str = (
                    date_str.split("T")[0] + "T" + date_str.split("T")[1].split(".")[0]
                )
                return datetime.fromisoformat(date_str.replace("Z", "+00:00"))
            else:
                return datetime.fromisoformat(date_str)
        except Exception as e:
            logger.warning(f"Erreur parsing date '{date_str}': {e}")
            return datetime.now()

    async def analyser_competences_mongodb(
        self, limite_offres: int | None = None, toutes_offres: bool = True
    ) -> dict[str, Any]:
        """
        Analyse les comp√©tences des offres stock√©es dans MongoDB

        Args:
            limite_offres: Nombre max d'offres √† analyser
            toutes_offres: Si True, analyse toutes les offres (pour time series)
                          Si False, seulement les 30 derniers jours

        Returns:
            R√©sultats de l'analyse
        """
        logger.info("D√©but analyse des comp√©tences depuis MongoDB")

        # Choix du scope d'analyse
        if toutes_offres:
            logger.info(
                "üïê Analyse de TOUTES les offres (s√©ries temporelles long terme)"
            )
            offres_non_traitees = await self.offres_repo.get_toutes_offres(
                limit=limite_offres or 10000
            )
        else:
            logger.info("üïê Analyse des offres r√©centes (30 derniers jours)")
            offres_non_traitees = await self.offres_repo.get_offres_recentes(
                jours=30, limit=limite_offres or 1000
            )

        if not offres_non_traitees:
            logger.warning("Aucune offre √† analyser dans MongoDB")
            return {"nb_offres_analysees": 0, "competences_detectees": {}}

        logger.info(f"Analyse de {len(offres_non_traitees)} offres")

        # Convertir les offres MongoDB en format pour l'analyzer
        offres_pour_analyse = []
        for offre in offres_non_traitees:
            offres_pour_analyse.append(
                {
                    "id": offre.get("source_id"),
                    "intitule": offre.get("intitule", ""),
                    "description": offre.get("description", ""),
                }
            )

        # Analyser avec l'analyzer existant
        resultats = self.analyzer.analyser_offres(offres_pour_analyse, verbose=True)

        # Sauvegarder les comp√©tences d√©tect√©es en MongoDB
        await self._sauvegarder_competences_detectees(resultats)

        # Mettre √† jour les offres avec les comp√©tences
        await self._mettre_a_jour_offres_avec_competences(resultats)

        # Marquer la derni√®re ex√©cution de l'analyse des comp√©tences
        await self.db.pipeline_executions.replace_one(
            {"type": "analyse_competences"},
            {
                "type": "analyse_competences",
                "derniere_execution": datetime.now(),
                "nb_offres_analysees": len(offres_non_traitees),
            },
            upsert=True,
        )

        logger.info("Analyse des comp√©tences termin√©e")
        return resultats

    async def _sauvegarder_competences_detectees(self, resultats: dict[str, Any]):
        """
        Sauvegarde les comp√©tences d√©tect√©es dans la collection competences_detections

        Args:
            resultats: R√©sultats de l'analyse des comp√©tences
        """
        detections = []

        for offre_id, competences in resultats.get("competences_par_offre", {}).items():
            for competence in competences:
                detection = {
                    "offre_id": offre_id,
                    "competence": competence.get("nom", ""),
                    "methode_detection": "nlp",  # M√©thode utilis√©e par l'analyzer
                    "confiance": competence.get("score", 1.0),
                    "contexte": competence.get("contexte", ""),
                    "date_detection": datetime.now(),
                }
                detections.append(detection)

        if detections:
            # Ins√©rer dans la collection competences_detections
            collection = self.db.competences_detections
            await collection.insert_many(detections)
            logger.info(f"{len(detections)} d√©tections de comp√©tences sauvegard√©es")

    async def _mettre_a_jour_offres_avec_competences(self, resultats: dict[str, Any]):
        """
        Met √† jour les offres avec les comp√©tences d√©tect√©es

        Args:
            resultats: R√©sultats de l'analyse
        """
        for offre_id, competences in resultats.get("competences_par_offre", {}).items():
            competences_noms = [comp.get("nom", "") for comp in competences]

            # Mettre √† jour l'offre dans MongoDB
            await self.offres_repo.collection.update_one(
                {"source_id": offre_id},
                {
                    "$set": {
                        "competences_extraites": competences_noms,
                        "traite": True,
                        "date_traitement": datetime.now(),
                    }
                },
            )

    async def generer_statistiques(self) -> dict[str, Any]:
        """
        G√©n√®re les statistiques compl√®tes depuis MongoDB

        Returns:
            Statistiques compl√®tes
        """
        logger.info("G√©n√©ration des statistiques MongoDB")

        # Statistiques g√©n√©rales
        stats_collection = await self.offres_repo.get_collection_stats()

        # Top comp√©tences
        pipeline_top_competences = [
            {"$unwind": "$competences_extraites"},
            {"$group": {"_id": "$competences_extraites", "count": {"$sum": 1}}},
            {"$sort": {"count": -1}},
            {"$limit": 20},
        ]

        cursor = self.offres_repo.collection.aggregate(pipeline_top_competences)
        top_competences = await cursor.to_list(length=20)

        # √âvolution temporelle (derniers 30 jours)
        stats_temporelles = await self.offres_repo.get_stats_temporelles(groupby="day")

        statistiques = {
            "periode_analysee": datetime.now().strftime("%Y-%m"),
            "date_analyse": datetime.now(),
            "nb_offres_analysees": stats_collection.get("total_offres", 0),
            "top_competences": [
                {
                    "competence": item["_id"],
                    "nb_occurrences": item["count"],
                    "pourcentage": round(
                        item["count"] / stats_collection.get("total_offres", 1) * 100, 2
                    ),
                }
                for item in top_competences
            ],
            "evolution_temporelle": stats_temporelles,
            "repartition_mensuelle": stats_collection.get("repartition_mensuelle", []),
        }

        # Sauvegarder les statistiques dans MongoDB
        await self.stats_repo.collection_stats.replace_one(
            {"periode_analysee": statistiques["periode_analysee"]},
            statistiques,
            upsert=True,
        )

        # Marquer la derni√®re ex√©cution des statistiques
        await self.db.pipeline_executions.replace_one(
            {"type": "statistiques"},
            {
                "type": "statistiques",
                "derniere_execution": datetime.now(),
                "nb_offres_analysees": statistiques["nb_offres_analysees"],
            },
            upsert=True,
        )

        logger.info("Statistiques g√©n√©r√©es et sauvegard√©es")
        return statistiques

    async def executer_pipeline_complet(
        self,
        max_offres: int | None = None,
        forcer_execution: bool = False,
        forcer_analyses: bool = False,
    ) -> dict[str, Any]:
        """
        Ex√©cute le pipeline complet MongoDB

        Args:
            max_offres: Limite d'offres √† collecter
            forcer_execution: Ignorer la v√©rification des 24h pour la collecte
            forcer_analyses: Ignorer la v√©rification des 24h pour analyses et stats

        Returns:
            R√©sultats complets du pipeline
        """
        start_time = datetime.now()
        logger.info(f"üöÄ D√©but pipeline MongoDB M1805 - Max offres: {max_offres}")

        try:
            # Initialiser la connexion MongoDB
            if not await self.init_database_connection():
                raise Exception("Impossible d'initialiser MongoDB")

            # V√©rifier la derni√®re ex√©cution
            if not forcer_execution:
                verification = await self.verifier_derniere_execution()
                if not verification["doit_executer"]:
                    logger.info(f"Pipeline ignor√©: {verification['raison']}")
                    return {
                        "success": True,
                        "skipped": True,
                        "reason": verification["raison"],
                        "derniere_execution": verification["derniere_execution"],
                    }

            # 1. Collecte des offres
            offres_api = await self.collecter_offres(max_offres)
            if not offres_api:
                raise Exception("Aucune offre collect√©e")

            # 2. Conversion et sauvegarde
            nb_sauvegardees = await self.convertir_et_sauvegarder_offres(offres_api)

            # 3. Analyse des comp√©tences (avec v√©rification 24h)
            verification_analyse = await self.verifier_derniere_analyse_competences()
            if verification_analyse["doit_analyser"] or forcer_analyses:
                logger.info("üß† D√©but analyse des comp√©tences")
                resultats_analyse = await self.analyser_competences_mongodb()
            else:
                logger.info(
                    f"‚è≠Ô∏è Analyse des comp√©tences ignor√©e: {verification_analyse['raison']}"
                )
                resultats_analyse = {
                    "nb_offres_analysees": 0,
                    "competences_detectees": {},
                    "skipped": True,
                }

            # 4. G√©n√©ration des statistiques (avec v√©rification 24h)
            verification_stats = await self.verifier_dernieres_statistiques()
            if verification_stats["doit_generer"] or forcer_analyses:
                logger.info("üìä D√©but g√©n√©ration des statistiques")
                statistiques = await self.generer_statistiques()
            else:
                logger.info(
                    f"‚è≠Ô∏è G√©n√©ration des statistiques ignor√©e: {verification_stats['raison']}"
                )
                # R√©cup√©rer les derni√®res statistiques existantes
                dernieres_stats = await self.stats_repo.collection_stats.find_one(
                    {}, sort=[("date_analyse", -1)]
                )
                statistiques = (
                    dernieres_stats
                    if dernieres_stats
                    else {"top_competences": [], "skipped": True}
                )

            # R√©sultats finaux
            duree = datetime.now() - start_time
            resultats = {
                "success": True,
                "skipped": False,
                "duree_execution": str(duree),
                "nb_offres_collectees": len(offres_api),
                "nb_offres_sauvegardees": nb_sauvegardees,
                "nb_offres_analysees": resultats_analyse.get("nb_offres_analysees", 0),
                "nb_competences_detectees": len(
                    resultats_analyse.get("competences_detectees", {})
                ),
                "top_competences": statistiques.get("top_competences", [])[:10],
                "analyse_competences_skipped": resultats_analyse.get("skipped", False),
                "statistiques_skipped": statistiques.get("skipped", False),
                "timestamp": datetime.now().isoformat(),
            }

            logger.info(f"‚úÖ Pipeline termin√© avec succ√®s en {duree}")
            return resultats

        except Exception as e:
            logger.error(f"‚ùå Erreur pipeline: {e}")
            return {
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
            }

        # Note: Fermeture g√©r√©e par main.py pour √©viter la double fermeture

    async def verifier_collecte_recente(
        self, heures_limite: int = 24
    ) -> dict[str, Any]:
        """
        V√©rifie s'il y a eu une collecte r√©cente dans MongoDB

        Args:
            heures_limite: Nombre d'heures pour consid√©rer une collecte comme r√©cente

        Returns:
            Dict avec informations sur la derni√®re collecte
        """
        try:
            from datetime import timedelta

            # Assurer la connexion MongoDB
            if not await self.init_database_connection():
                return {"doit_collecter": True, "raison": "Pas de connexion MongoDB"}

            # Chercher la collecte la plus r√©cente
            dernier_document = await self.offres_repo.collection.find_one(
                sort=[("date_collecte", -1)]
            )

            if not dernier_document:
                return {"doit_collecter": True, "raison": "Aucune donn√©e en base"}

            date_limite = datetime.now() - timedelta(hours=heures_limite)
            date_collecte = dernier_document.get("date_collecte")

            if not date_collecte or date_collecte < date_limite:
                return {
                    "doit_collecter": True,
                    "raison": f"Derni√®re collecte trop ancienne ({date_collecte})",
                    "date_collecte": date_collecte,
                }

            # Compter les offres r√©centes
            nb_offres_recentes = await self.offres_repo.collection.count_documents(
                {"date_collecte": {"$gte": date_limite}}
            )

            return {
                "doit_collecter": False,
                "raison": f"Collecte r√©cente trouv√©e ({nb_offres_recentes} offres)",
                "date_collecte": date_collecte,
                "nb_offres": nb_offres_recentes,
            }

        except Exception as e:
            self.logger.warning(f"Erreur v√©rification collecte r√©cente: {e}")
            return {"doit_collecter": True, "raison": "Erreur v√©rification"}

    async def obtenir_statistiques_mongodb(self) -> dict[str, Any]:
        """
        Obtient les statistiques compl√®tes depuis MongoDB

        Returns:
            Dict avec toutes les statistiques disponibles
        """
        try:
            # Assurer la connexion MongoDB
            if not await self.init_database_connection():
                return {"error": "Pas de connexion MongoDB"}

            # Statistiques des offres
            nb_offres_total = await self.offres_repo.collection.count_documents({})
            nb_offres_traitees = await self.offres_repo.collection.count_documents(
                {"traite": True}
            )

            # Derni√®re collecte
            derniere_collecte = await self.offres_repo.collection.find_one(
                sort=[("date_collecte", -1)]
            )

            # Statistiques des comp√©tences
            nb_competences_total = (
                await self.competences_repo.collection.count_documents({})
            )
            nb_detections_total = await self.db.competences_detections.count_documents(
                {}
            )

            # Top comp√©tences
            pipeline_top_competences = [
                {"$unwind": "$competences_extraites"},
                {"$group": {"_id": "$competences_extraites", "count": {"$sum": 1}}},
                {"$sort": {"count": -1}},
                {"$limit": 5},
            ]
            cursor = self.offres_repo.collection.aggregate(pipeline_top_competences)
            top_competences = await cursor.to_list(length=5)

            # R√©partition par d√©partement (top 5)
            pipeline_dept = [
                {"$group": {"_id": "$localisation.departement", "count": {"$sum": 1}}},
                {"$sort": {"count": -1}},
                {"$limit": 5},
            ]
            cursor_dept = self.offres_repo.collection.aggregate(pipeline_dept)
            top_departements = await cursor_dept.to_list(length=5)

            stats = {
                "code_rome": "M1805",
                "nb_offres_total": nb_offres_total,
                "nb_offres_traitees": nb_offres_traitees,
                "nb_competences_total": nb_competences_total,
                "nb_detections_total": nb_detections_total,
                "derniere_collecte": (
                    derniere_collecte.get("date_collecte")
                    if derniere_collecte
                    else None
                ),
                "top_competences": [
                    {"competence": item["_id"], "occurrences": item["count"]}
                    for item in top_competences
                    if item["_id"]
                ],
                "top_departements": [
                    {"departement": item["_id"], "offres": item["count"]}
                    for item in top_departements
                    if item["_id"]
                ],
                "stockage": "MongoDB",
                "version_pipeline": "MongoDB v2.0",
            }

            return stats

        except Exception as e:
            self.logger.error(f"Erreur obtention statistiques MongoDB: {e}")
            return {
                "code_rome": "M1805",
                "nb_offres_total": 0,
                "nb_competences_total": 0,
                "stockage": "MongoDB (erreur)",
                "error": str(e),
            }


# Fonctions d'interface pour compatibilit√©
async def run_pipeline_mongodb(
    max_offres: int | None = None, forcer_execution: bool = False
) -> dict[str, Any]:
    """
    Lance le pipeline MongoDB M1805

    Args:
        max_offres: Limite d'offres
        forcer_execution: Ignorer la v√©rification 24h

    Returns:
        R√©sultats du pipeline
    """
    pipeline = PipelineMongoDBM1805()
    return await pipeline.executer_pipeline_complet(max_offres, forcer_execution)


def run_pipeline_mongodb_sync(
    max_offres: int | None = None, forcer_execution: bool = False
) -> dict[str, Any]:
    """
    Version synchrone du pipeline MongoDB (wrapper)

    Args:
        max_offres: Limite d'offres
        forcer_execution: Ignorer la v√©rification 24h

    Returns:
        R√©sultats du pipeline
    """
    return asyncio.run(run_pipeline_mongodb(max_offres, forcer_execution))


if __name__ == "__main__":
    # Test du pipeline MongoDB
    import sys

    max_offres = int(sys.argv[1]) if len(sys.argv) > 1 else 50
    resultats = run_pipeline_mongodb_sync(max_offres=max_offres, forcer_execution=True)

    print("\n" + "=" * 60)
    print("üéØ R√âSULTATS PIPELINE MONGODB M1805")
    print("=" * 60)

    if resultats.get("success"):
        print(f"‚úÖ Succ√®s - Dur√©e: {resultats.get('duree_execution')}")
        print(f"üìä Offres collect√©es: {resultats.get('nb_offres_collectees')}")
        print(f"üíæ Offres sauvegard√©es: {resultats.get('nb_offres_sauvegardees')}")
        print(f"üß† Offres analys√©es: {resultats.get('nb_offres_analysees')}")
        print(f"üîç Comp√©tences d√©tect√©es: {resultats.get('nb_competences_detectees')}")

        if resultats.get("top_competences"):
            print("\nüèÜ Top 5 comp√©tences:")
            for i, comp in enumerate(resultats["top_competences"][:5], 1):
                print(
                    f"   {i}. {comp['competence']} - {comp['nb_occurrences']} occurrences"
                )
    else:
        print(f"‚ùå √âchec: {resultats.get('error')}")
