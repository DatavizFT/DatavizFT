"""
Pipeline France Travail M1805 - Version refactoris√©e
Collecte et analyse des offres d'emploi pour le code ROME M1805 (√âtudes et d√©veloppement informatique)
"""

import os
import glob
from datetime import datetime, timedelta
from typing import Dict, Any, Optional
from ..tools.data_loader import charger_competences_referentiel, charger_config_pipeline
from ..tools.api_client import FranceTravailAPIClient
from ..tools.competence_analyzer import CompetenceAnalyzer
from ..tools.file_manager import FileManager


class PipelineM1805:
    """Pipeline de collecte et d'analyse pour les offres M1805"""
    
    def __init__(self):
        """Initialise le pipeline avec ses composants"""
        self.config = charger_config_pipeline("france_travail_m1805")
        self.competences_referentiel = charger_competences_referentiel()
        
        # Initialisation des composants
        self.api_client = FranceTravailAPIClient(
            user_agent=self.config.get("user_agent", "DatavizFT-Collector/1.0"),
            rate_limit_ms=self.config.get("rate_limit_ms", 120)
        )
        
        self.analyzer = CompetenceAnalyzer(self.competences_referentiel)
        self.file_manager = FileManager()
        
        # Configuration
        self.code_rome = self.config.get("code_rome", "M1805")
        
        print(f"üöÄ Pipeline M1805 initialis√©")
        print(f"üìã {len(self.competences_referentiel)} cat√©gories de comp√©tences charg√©es")
    
    def verifier_derniere_execution(self) -> Dict[str, Any]:
        """
        V√©rifie si le pipeline a √©t√© ex√©cut√© dans les 24 derni√®res heures
        
        Returns:
            Dict contenant le statut et les informations de la derni√®re ex√©cution
        """
        try:
            # Rechercher les fichiers d'offres M1805 dans le dossier data
            pattern = "data/offres_M1805_FRANCE_*.json"
            fichiers_offres = glob.glob(pattern)
            
            if not fichiers_offres:
                return {
                    "doit_executer": True,
                    "raison": "Aucun fichier d'offres trouv√©",
                    "dernier_fichier": None,
                    "derniere_execution": None
                }
            
            # Trier par nom (le timestamp est dans le nom) pour avoir le plus r√©cent
            fichiers_offres.sort(reverse=True)
            dernier_fichier = fichiers_offres[0]
            
            # Extraire le timestamp du nom de fichier (format: YYYYMMDD_HHMMSS)
            nom_fichier = os.path.basename(dernier_fichier)
            # Format: offres_M1805_FRANCE_20251005_091707.json
            parties = nom_fichier.split('_')
            if len(parties) >= 4:
                date_str = parties[3]  # 20251005
                heure_str = parties[4].split('.')[0]  # 091707
                
                # Convertir en datetime
                timestamp_str = f"{date_str}_{heure_str}"
                derniere_execution = datetime.strptime(timestamp_str, "%Y%m%d_%H%M%S")
                
                # V√©rifier si c'est dans les 24 derni√®res heures
                maintenant = datetime.now()
                difference = maintenant - derniere_execution
                
                if difference < timedelta(hours=24):
                    return {
                        "doit_executer": False,
                        "raison": f"Ex√©cution r√©cente il y a {difference}",
                        "dernier_fichier": dernier_fichier,
                        "derniere_execution": derniere_execution,
                        "heures_restantes": 24 - difference.total_seconds() / 3600
                    }
                else:
                    return {
                        "doit_executer": True,
                        "raison": f"Derni√®re ex√©cution il y a {difference} (> 24h)",
                        "dernier_fichier": dernier_fichier,
                        "derniere_execution": derniere_execution
                    }
            else:
                return {
                    "doit_executer": True,
                    "raison": "Format de fichier non reconnu",
                    "dernier_fichier": dernier_fichier,
                    "derniere_execution": None
                }
                
        except Exception as e:
            return {
                "doit_executer": True,
                "raison": f"Erreur lors de la v√©rification: {e}",
                "dernier_fichier": None,
                "derniere_execution": None
            }
    
    def collecter_offres(self, max_offres: Optional[int] = None) -> list:
        """
        Collecte les offres d'emploi M1805
        
        Args:
            max_offres: Limite optionnelle du nombre d'offres
        
        Returns:
            Liste des offres collect√©es
        """
        print(f"\nüîç COLLECTE DES OFFRES {self.code_rome}")
        print("=" * 50)
        
        offres = self.api_client.collecter_offres_par_code_rome(
            self.code_rome,
            max_offres=max_offres
        )
        
        print(f"‚úÖ {len(offres)} offres collect√©es")
        return offres
    
    def analyser_competences(self, offres: list) -> Dict[str, Any]:
        """
        Analyse les comp√©tences dans les offres
        
        Args:
            offres: Liste des offres √† analyser
        
        Returns:
            R√©sultats de l'analyse
        """
        print(f"\nüéØ ANALYSE DES COMP√âTENCES")
        print("=" * 50)
        
        resultats = self.analyzer.analyser_offres(offres, verbose=True)
        
        print(f"‚úÖ Analyse termin√©e")
        return resultats
    
    def sauvegarder_donnees(self, offres: list, resultats_analyse: Dict[str, Any]) -> Dict[str, str]:
        """
        Sauvegarde les offres et les r√©sultats d'analyse
        
        Args:
            offres: Liste des offres
            resultats_analyse: R√©sultats de l'analyse
        
        Returns:
            Dict avec les chemins des fichiers sauvegard√©s
        """
        print(f"\nüíæ SAUVEGARDE DES DONN√âES")
        print("=" * 50)
        
        # Cr√©er la structure de dossiers
        self.file_manager.creer_structure_dossiers()
        
        # Sauvegarder les offres
        chemin_offres = self.file_manager.sauvegarder_offres(offres, self.code_rome)
        
        # Sauvegarder l'analyse compl√®te
        chemin_analyse = self.file_manager.sauvegarder_analyse_competences(
            resultats_analyse, self.code_rome
        )
        
        # Sauvegarder les comp√©tences enrichies
        chemin_competences = self.file_manager.sauvegarder_competences_enrichies(
            resultats_analyse['resultats_par_categorie'],
            resultats_analyse['nb_offres_analysees'],
            self.code_rome
        )
        
        chemins = {
            "offres": chemin_offres,
            "analyse": chemin_analyse, 
            "competences": chemin_competences
        }
        
        print(f"‚úÖ Toutes les donn√©es sauvegard√©es")
        return chemins
    
    def executer_pipeline_complet(self, max_offres: Optional[int] = None) -> Dict[str, Any]:
        """
        Ex√©cute le pipeline complet de collecte et d'analyse
        
        Args:
            max_offres: Limite optionnelle du nombre d'offres
        
        Returns:
            Dict avec les r√©sultats et informations du pipeline
        """
        print(f"üöÄ EX√âCUTION DU PIPELINE COMPLET M1805")
        print("=" * 70)
        
        try:
            # 1. Collecte des offres
            offres = self.collecter_offres(max_offres)
            
            if not offres:
                return {
                    "success": False,
                    "error": "Aucune offre collect√©e",
                    "nb_offres": 0
                }
            
            # 2. Analyse des comp√©tences
            resultats_analyse = self.analyser_competences(offres)
            
            # 3. Sauvegarde
            chemins_fichiers = self.sauvegarder_donnees(offres, resultats_analyse)
            
            # 4. R√©sum√© final
            print(f"\nüéâ PIPELINE TERMIN√â AVEC SUCC√àS")
            print("=" * 70)
            print(f"üìä {len(offres)} offres M1805 collect√©es et analys√©es")
            print(f"üìÅ Fichiers g√©n√©r√©s dans le dossier data/")
            
            return {
                "success": True,
                "nb_offres": len(offres),
                "nb_competences_detectees": sum(
                    len(cat['competences']) 
                    for cat in resultats_analyse['resultats_par_categorie'].values()
                ),
                "fichiers_generes": chemins_fichiers,
                "resultats_analyse": resultats_analyse
            }
            
        except Exception as e:
            print(f"\n‚ùå ERREUR PIPELINE: {e}")
            return {
                "success": False,
                "error": str(e),
                "nb_offres": 0
            }
    
    def obtenir_statistiques_pipeline(self) -> Dict[str, Any]:
        """
        Obtient les statistiques du pipeline et des donn√©es
        
        Returns:
            Dict avec les statistiques compl√®tes
        """
        stats_stockage = self.file_manager.obtenir_statistiques_stockage()
        
        return {
            "config": self.config,
            "code_rome": self.code_rome,
            "nb_categories_competences": len(self.competences_referentiel),
            "nb_competences_total": sum(len(comp) for comp in self.competences_referentiel.values()),
            "stockage": stats_stockage
        }


# Fonction principale pour compatibilit√© avec l'ancien syst√®me
def run_pipelineFT(forcer_execution: bool = False) -> Dict[str, Any]:
    """
    Point d'entr√©e principal du pipeline - maintient la compatibilit√©
    V√©rifie automatiquement si une ex√©cution r√©cente existe (< 24h)
    
    Args:
        forcer_execution: Si True, force l'ex√©cution m√™me si r√©cente
    
    Returns:
        R√©sultats de l'ex√©cution du pipeline
    """
    pipeline = PipelineM1805()
    
    # V√©rifier la derni√®re ex√©cution si pas de for√ßage
    if not forcer_execution:
        verification = pipeline.verifier_derniere_execution()
        
        if not verification["doit_executer"]:
            print(f"‚è≠Ô∏è EX√âCUTION IGNOR√âE: {verification['raison']}")
            if verification.get('heures_restantes'):
                heures = verification['heures_restantes']
                print(f"‚è∞ Prochaine ex√©cution possible dans {heures:.1f} heures")
            print(f"üìÅ Dernier fichier: {os.path.basename(verification['dernier_fichier'])}")
            print("üí° Utilisez forcer_execution=True pour ex√©cuter quand m√™me")
            
            return {
                "success": True,
                "skipped": True,
                "nb_offres": 0,
                "raison": verification["raison"],
                "derniere_execution": verification.get("derniere_execution"),
                "dernier_fichier": verification.get("dernier_fichier")
            }
        else:
            print(f"‚úÖ EX√âCUTION AUTORIS√âE: {verification['raison']}")
    else:
        print("üî• EX√âCUTION FORC√âE (ignorant la v√©rification des 24h)")
    
    return pipeline.executer_pipeline_complet()


# Fonction pour ex√©cution avec param√®tres
def run_pipeline_avec_limite(max_offres: int) -> Dict[str, Any]:
    """
    Ex√©cute le pipeline avec une limite d'offres
    
    Args:
        max_offres: Nombre maximum d'offres √† collecter
    
    Returns:
        R√©sultats de l'ex√©cution du pipeline
    """
    pipeline = PipelineM1805()
    return pipeline.executer_pipeline_complet(max_offres=max_offres)


if __name__ == "__main__":
    # Test direct du pipeline
    resultat = run_pipelineFT()
    if resultat["success"]:
        if resultat.get("skipped"):
            print(f"‚è≠Ô∏è Pipeline ignor√©: {resultat.get('raison', 'Ex√©cution r√©cente')}")
        else:
            print(f"‚úÖ Pipeline r√©ussi: {resultat['nb_offres']} offres trait√©es")
    else:
        print(f"‚ùå Pipeline √©chou√©: {resultat.get('error', 'Erreur inconnue')}")