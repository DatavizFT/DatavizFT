"""
Pipeline Orchestrator - Orchestrateur g√©n√©rique multi-sources
Coordonne l'ex√©cution des pipelines avec logique de throttling 24h
"""

import asyncio
from datetime import datetime, timedelta
from typing import Any

from motor.motor_asyncio import AsyncIOMotorDatabase

from .data_collector import BaseDataCollector
from .stats_generator import StatsGenerator
from ...database.connection import DatabaseConnection
from ...database.repositories import (
    CompetencesRepository, 
    OffresRepository, 
    StatsRepository
)
from ...tools.logging_config import get_logger

logger = get_logger(__name__)


class PipelineOrchestrator:
    """
    Orchestrateur g√©n√©rique pour pipelines multi-sources
    G√®re la coordination, le throttling 24h et l'ex√©cution des √©tapes
    """
    
    def __init__(
        self,
        pipeline_name: str,
        data_collector: BaseDataCollector,
        stats_generator: StatsGenerator | None = None,
        database_connection: DatabaseConnection | None = None
    ):
        """
        Initialise l'orchestrateur
        
        Args:
            pipeline_name: Nom unique du pipeline (ex: "france_travail_m1805")
            data_collector: Collecteur de donn√©es sp√©cialis√©
            database_connection: Connexion MongoDB (optionnel)
        """
        self.pipeline_name = pipeline_name
        self.data_collector = data_collector
        self.stats_generator = stats_generator
        
        # Connexion base de donn√©es
        self.db_connection = database_connection or DatabaseConnection()
        self.db: AsyncIOMotorDatabase | None = None
        
        # Repositories
        self.offres_repo: OffresRepository | None = None
        self.competences_repo: CompetencesRepository | None = None 
        self.stats_repo: StatsRepository | None = None
        
        logger.info(f"Orchestrateur {pipeline_name} initialis√©")
    
    async def init_database(self) -> bool:
        """
        Initialise les connexions base de donn√©es
        
        Returns:
            True si succ√®s, False sinon
        """
        try:
            # Test de connexion
            if not await self.db_connection.connect():
                return False
            
            # R√©cup√©ration de la DB
            self.db = self.db_connection.async_db
            
            # Initialisation des repositories
            self.offres_repo = OffresRepository(self.db)
            self.competences_repo = CompetencesRepository(self.db) 
            self.stats_repo = StatsRepository(self.db)
            
            # Configuration du collecteur et g√©n√©rateur de stats
            self.data_collector.set_database(self.db)
            if self.stats_generator:
                self.stats_generator.set_database(self.db)
            
            logger.info(f"[OK] Base de donn√©es initialis√©e pour {self.pipeline_name}")
            return True
            
        except Exception as e:
            logger.error(f"[ERREUR] Erreur init DB {self.pipeline_name}: {e}")
            return False
    
    async def close_database(self):
        """Ferme les connexions base de donn√©es"""
        try:
            if self.db_connection:
                await self.db_connection.close()
                logger.info(f"[DB] Connexions ferm√©es pour {self.pipeline_name}")
        except Exception as e:
            logger.warning(f"Erreur fermeture DB {self.pipeline_name}: {e}")
    
    async def check_should_collect(self, hours_limit: int = 24) -> dict[str, Any]:
        """
        V√©rifie si la collecte doit √™tre ex√©cut√©e (throttling)
        
        Args:
            hours_limit: Limite en heures pour consid√©rer une collecte comme r√©cente
            
        Returns:
            Dictionnaire avec should_execute et informations
        """
        try:
            # V√©rifier la derni√®re collecte de cette source
            last_collection = await self.offres_repo.collection.find_one(
                {"source": self.data_collector.source_name},
                sort=[("date_collecte", -1)]
            )
            
            if not last_collection:
                return {
                    "should_execute": True,
                    "reason": f"Aucune collecte pr√©c√©dente pour {self.data_collector.source_name}",
                    "last_execution": None,
                }
            
            last_date = last_collection.get("date_collecte")
            if not last_date:
                return {
                    "should_execute": True,
                    "reason": "Date de collecte manquante",
                    "last_execution": None,
                }
            
            # V√©rifier le d√©lai
            now = datetime.now()
            if isinstance(last_date, str):
                last_date = datetime.fromisoformat(last_date)
            
            delta = now - last_date
            
            if delta <= timedelta(hours=hours_limit):
                return {
                    "should_execute": False,
                    "reason": f"Collecte r√©cente (il y a {delta.total_seconds()/3600:.1f}h)",
                    "last_execution": last_date,
                    "hours_since_last": delta.total_seconds() / 3600,
                }
            else:
                return {
                    "should_execute": True,
                    "reason": f"Collecte obsol√®te (il y a {delta.days} jours {delta.seconds//3600}h)",
                    "last_execution": last_date,
                    "hours_since_last": delta.total_seconds() / 3600,
                }
                
        except Exception as e:
            logger.error(f"Erreur v√©rification collecte {self.pipeline_name}: {e}")
            return {
                "should_execute": True,
                "reason": f"Erreur v√©rification: {e}",
                "last_execution": None,
            }
    
    async def check_should_analyze(self, hours_limit: int = 24) -> dict[str, Any]:
        """
        V√©rifie si l'analyse des comp√©tences doit √™tre ex√©cut√©e
        
        Args:
            hours_limit: Limite en heures
            
        Returns:
            Dictionnaire avec should_execute et informations
        """
        try:
            # Chercher la derni√®re analyse pour ce pipeline
            last_analysis = await self.db.pipeline_executions.find_one(
                {
                    "pipeline_name": self.pipeline_name,
                    "type": "analyse_competences"
                },
                sort=[("derniere_execution", -1)]
            )
            
            if not last_analysis:
                return {
                    "should_execute": True,
                    "reason": f"Aucune analyse pr√©c√©dente pour {self.pipeline_name}",
                    "last_execution": None,
                }
            
            last_date = last_analysis.get("derniere_execution")
            if not last_date:
                return {
                    "should_execute": True,
                    "reason": "Date d'analyse manquante",
                    "last_execution": None,
                }
            
            # V√©rifier le d√©lai
            now = datetime.now()
            if isinstance(last_date, str):
                last_date = datetime.fromisoformat(last_date)
            
            delta = now - last_date
            
            if delta <= timedelta(hours=hours_limit):
                return {
                    "should_execute": False,
                    "reason": f"Analyse r√©cente (il y a {delta.total_seconds()/3600:.1f}h)",
                    "last_execution": last_date,
                }
            else:
                return {
                    "should_execute": True,
                    "reason": f"Analyse obsol√®te (il y a {delta.days} jours)",
                    "last_execution": last_date,
                }
                
        except Exception as e:
            logger.error(f"Erreur v√©rification analyse {self.pipeline_name}: {e}")
            return {
                "should_execute": True,
                "reason": f"Erreur v√©rification: {e}",
                "last_execution": None,
            }
    
    async def check_should_generate_stats(self, hours_limit: int = 24) -> dict[str, Any]:
        """
        V√©rifie si la g√©n√©ration de statistiques doit √™tre ex√©cut√©e
        
        Args:
            hours_limit: Limite en heures
            
        Returns:
            Dictionnaire avec should_execute et informations  
        """
        try:
            # Chercher la derni√®re g√©n√©ration pour ce pipeline
            last_stats = await self.db.pipeline_executions.find_one(
                {
                    "pipeline_name": self.pipeline_name,
                    "type": "statistiques"
                },
                sort=[("derniere_execution", -1)]
            )
            
            if not last_stats:
                return {
                    "should_execute": True,
                    "reason": f"Aucune statistique pr√©c√©dente pour {self.pipeline_name}",
                    "last_execution": None,
                }
            
            last_date = last_stats.get("derniere_execution")
            if not last_date:
                return {
                    "should_execute": True,
                    "reason": "Date de g√©n√©ration manquante",
                    "last_execution": None,
                }
            
            # V√©rifier le d√©lai
            now = datetime.now()
            if isinstance(last_date, str):
                last_date = datetime.fromisoformat(last_date)
            
            delta = now - last_date
            
            if delta <= timedelta(hours=hours_limit):
                return {
                    "should_execute": False,
                    "reason": f"Statistiques r√©centes (il y a {delta.total_seconds()/3600:.1f}h)",
                    "last_execution": last_date,
                }
            else:
                return {
                    "should_execute": True,
                    "reason": f"Statistiques obsol√®tes (il y a {delta.days} jours)",
                    "last_execution": last_date,
                }
                
        except Exception as e:
            logger.error(f"Erreur v√©rification stats {self.pipeline_name}: {e}")
            return {
                "should_execute": True,
                "reason": f"Erreur v√©rification: {e}",
                "last_execution": None,
            }
    
    async def execute_complete_pipeline(
        self,
        max_jobs: int | None = None,
        force_collection: bool = False,
        force_analysis: bool = False,
        force_stats: bool = False,
        **collect_kwargs
    ) -> dict[str, Any]:
        """
        Ex√©cute le pipeline complet avec orchestration intelligente
        
        Args:
            max_jobs: Limite d'offres √† collecter
            force_collection: Ignorer throttling collecte
            force_analysis: Ignorer throttling analyse
            force_stats: Ignorer throttling statistiques
            **collect_kwargs: Param√®tres pour la collecte
            
        Returns:
            R√©sultats complets du pipeline
        """
        start_time = datetime.now()
        logger.info(f"D√©but pipeline {self.pipeline_name} - Max: {max_jobs}")
        
        try:
            # Initialiser la base de donn√©es
            if not await self.init_database():
                raise Exception("Impossible d'initialiser la base de donn√©es")
            
            # Variables de r√©sultats
            collection_results = {}
            analysis_results = {}
            stats_results = {}
            
            # 1. COLLECTE (avec throttling)
            collection_check = await self.check_should_collect()
            should_collect = collection_check["should_execute"] or force_collection
            
            if should_collect:
                logger.info(f"D√©but collecte {self.data_collector.source_name}")
                
                # V√©rifier que la m√©thode optimis√©e existe
                if not hasattr(self.data_collector, 'collect_and_save_jobs_optimized'):
                    raise RuntimeError(
                        f"Le collecteur {self.data_collector.source_name} doit impl√©menter "
                        "collect_and_save_jobs_optimized() pour le filtrage intelligent"
                    )
                
                collection_results = await self.data_collector.collect_and_save_jobs_optimized(
                    max_jobs, **collect_kwargs
                )
            else:
                logger.info(f"‚è≠Ô∏è Collecte ignor√©e: {collection_check['reason']}")
                collection_results = {
                    "success": True,
                    "skipped": True,
                    "reason": collection_check["reason"],
                    "source": self.data_collector.source_name,
                }
            
            # 2. ANALYSE (avec throttling)
            analysis_check = await self.check_should_analyze()
            should_analyze = analysis_check["should_execute"] or force_analysis
            
            if should_analyze:
                logger.info(f"üß† D√©but analyse des comp√©tences {self.data_collector.source_name}")
                
                # Cr√©er le processeur de comp√©tences
                from .competence_processor import CompetenceProcessor
                competence_processor = CompetenceProcessor(self.data_collector.source_name)
                competence_processor.set_database(self.db)
                
                # Lancer l'analyse
                analysis_results = await competence_processor.analyze_jobs_competences(
                    pipeline_name=self.pipeline_name,
                    limit_jobs=None,  # Pas de limite
                    analyze_all=True,  # Analyser tous les jobs
                    force_reanalyze=force_analysis
                )
            else:
                logger.info(f"‚è≠Ô∏è Analyse ignor√©e: {analysis_check['reason']}")
                analysis_results = {
                    "success": True, 
                    "skipped": True,
                    "reason": analysis_check["reason"]
                }
            
            # 3. STATISTIQUES (avec throttling) 
            stats_check = await self.check_should_generate_stats()
            should_generate_stats = stats_check["should_execute"] or force_stats
            
            if should_generate_stats and self.stats_generator:
                logger.info(f"D√©but g√©n√©ration statistiques {self.stats_generator.source_name}")
                stats_results = await self.stats_generator.generate_complete_statistics(
                    pipeline_name=self.pipeline_name,
                    include_temporal=True,
                    include_categories=True
                )
            elif should_generate_stats:
                logger.warning(f"‚ö†Ô∏è StatsGenerator non configur√© pour {self.pipeline_name}")
                stats_results = {
                    "success": False,
                    "error": "StatsGenerator non configur√©",
                    "source": self.data_collector.source_name
                }
            else:
                logger.info(f"‚è≠Ô∏è Statistiques ignor√©es: {stats_check['reason']}")
                stats_results = {
                    "success": True,
                    "skipped": True, 
                    "reason": stats_check["reason"]
                }
            
            # R√©sultats finaux
            duration = datetime.now() - start_time
            results = {
                "success": True,
                "pipeline_name": self.pipeline_name,
                "source": self.data_collector.source_name,
                "duration": str(duration),
                "timestamp": datetime.now().isoformat(),
                
                # R√©sultats par √©tape
                "collection": collection_results,
                "analysis": analysis_results,
                "statistics": stats_results,
                
                # Flags d'ex√©cution
                "collection_executed": should_collect,
                "analysis_executed": should_analyze,
                "stats_executed": should_generate_stats,
            }
            
            logger.info(f"[OK] Pipeline {self.pipeline_name} termin√© en {duration}")
            return results
            
        except Exception as e:
            logger.error(f"[ERREUR] Erreur pipeline {self.pipeline_name}: {e}")
            return {
                "success": False,
                "pipeline_name": self.pipeline_name,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
            }
        
        finally:
            # Nettoyage des connexions
            await self.close_database()